Tensor: Array with more than 2 axes.

Identity and inverse matrices
can soolve Ax=b, by x=A-1b

for A-1, AA-1=I must have one solution
Its possible for equations to have any of infinite solutions
sum(XiAi), where columnns of Ai is the diff directions to reach b, and x is how far to reach b, sum of it will give b

Span: It is set of all points obtainable by linear combination of original vectoors
For Ax-b, to have solution b must be in span of A, this is only possible if n>=m, i.e. dimenesion of A >=be

It is possible 2 columns in A are identical, so its same as copy of one column, this is linear dependence.
A set of vectors is linearly independent if no vector in the set is a linear combintaion of other vectors.
So matrix must have m linearly independent columns.

For Ax=b, to have an inverse there must be atmost one solution for each value of b. so n=m
So it must be a square matrix n=m and all columns independent.

Singular matrix is a square matrix with linearly independent columns.

NORMS

In Ml, we measure size of vectors using a functioon called norm. L^p norm is given by (sum(x^p))^(1/p)
Norm of vextor x, measures distance from origin to point x.
L^2 is Euclidean norm i.e. Euclidean distance from origin. ||x||subscript{{2}}

L^2 can be calculated as x^t.x
L^2 norm maybe undesirable because it increases very slowly near the origin, so we turn to L1 norm.
It is used when difference between 0 and non0 elements is very important.

Diagonal matrix
The non-diagonal elements are 0. It is important because multiplying by a diagonal matrix very efficient.

Symmentrix matrix: A=At
Unit vector: vector with unit norm

Orthogonal: if xty=0, they are 90 degree to each other.
Orthonormal: if vectors are orthogonal and have unit norm

Orthogonal matrices: Is a square matrix whose rows are mutually Orthonormal
AAt=I, so At=A-1

EIGENDECOMPOSITION
We can decompose an integer into prime factors, can get info about divisibilty
Similarly we can decompse a matrix
Eigen decomposition is where we decompose a matrix into eigen vectors and eigen values

Eigen vector: Eigen vector v of a square matrix A is a nonzeroo vector v, such that multiplication with A only alters scale of v
Av = lambda.v
The lambda is the eigen value

If A is the combination of n independent eigen vectors, with corresponding eigenvalues,
 We can concatenate all vs to form V and lambdas to form lambda
 Eigen decomposition of A = V.diag(lambda).V-1
Constructing matrices with specific eigenvalues and eigenvectors enable to stretch space in desired directions

For a symmentric matric A=QVQt
where Q is orthogonal matricz composed oof eigenvectors of A, diagonal matrix vi,i is associated with eigenvector column i of Q
Eigendecomposation can be made unique by considering that the elements of v is always in descending order.
 
If all eigenvalues are zero, it is a singular matrix
If eigenvalues are > 0, positive definite, >=0, positive semidefinite

SINGULAR VALUE DECOMPOSITION
-It is a way to factorize matrix into singular vectors and singular values.
Every matrix has SVD unlike EVD 

TRACE OPERATOR
-It gives sum of all diagonal entries of a matrix

DETERMINANT
-It is a function that maps matrics to real scalars. It is equal to product of all eigenvalues of matrix
-The absolute value of determinant can be thoought as  a measure of how much multiplication by matrics expands or constracts space
-If 0, contracted completely, if 1, volume is preserved

PCA: Principal Component Analysis
-can be derived using linear algebra
-If we have collection of m points and want to apply lossy compresion i.e. storing info using less memory
-Encode info in lower dimension, for each point in Rn, we find a encoding function to produce a point in Rl, l<n
 and a decoding function to get back original info
-PCA is defined by the choice of decoding function, to make it simple we use a matrix
-To keep it easy, only orthogonal matrix is considered, different scales of msatrix possible, so its constrained
 to keep each column to have unit norm decoder g(c) = Dc
-To find our optimal code point c for each input, we minimize distance between x and g(c) using L2 norm
-can also do of square of l2 norm i.e.(x-g(c))t(x-g(c)), after removing terms w/o c, coz min wrt c,
to min min, we differentiate wrt c and =0. upon this and substrituoon of g as Dc, the final eq is 
 c = Dtx

